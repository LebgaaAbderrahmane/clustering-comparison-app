"""
Application Streamlit pour la Comparaison d'Algorithmes de Clustering
=====================================================================
Auteur: Expert en Data Mining
Version: 1.0
Python: 3.10+

D√©pendances requises:
pip install streamlit scikit-learn pandas plotly seaborn scipy openpyxl
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from sklearn import datasets
# from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.preprocessing import StandardScaler
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import linkage, fcluster
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import seaborn as sns
from io import BytesIO

# Configuration de la page
st.set_page_config(
    page_title="Comparaison Clustering",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Initialisation de session_state
if 'results_history' not in st.session_state:
    st.session_state.results_history = []
if 'current_data' not in st.session_state:
    st.session_state.current_data = None


if 'scaling_applied' not in st.session_state:
    st.session_state.scaling_applied = False
if 'preprocessing_done' not in st.session_state:
    st.session_state.preprocessing_done = False

if 'unscaled_data' not in st.session_state:
    st.session_state.unscaled_data = None

if 'data_before_scaling' not in st.session_state:
    st.session_state.data_before_scaling = None
    
# ============================================================================
# FONCTIONS UTILITAIRES
# ============================================================================

class KMedoids:
    """K-Medoids clustering algorithm"""
    def __init__(self, n_clusters=3, max_iter=100, random_state=42):
        self.n_clusters = n_clusters
        self.max_iter = max_iter
        self.random_state = random_state
        self.medoids = None
        self.labels_ = None
    
    def fit_predict(self, X):
        np.random.seed(self.random_state)
        n_samples = X.shape[0]
        
        # Initialize medoids randomly
        medoid_indices = np.random.choice(n_samples, self.n_clusters, replace=False)
        self.medoids = X[medoid_indices]
        
        # Compute distance matrix
        distances = squareform(pdist(X, metric='euclidean'))
        
        for iteration in range(self.max_iter):
            # Assign points to nearest medoid
            medoid_distances = distances[medoid_indices]
            self.labels_ = np.argmin(medoid_distances, axis=0)
            
            # Update medoids
            new_medoids = medoid_indices.copy()
            for k in range(self.n_clusters):
                cluster_mask = self.labels_ == k
                if cluster_mask.sum() > 0:
                    cluster_indices = np.where(cluster_mask)[0]
                    # Find point with minimum average distance to others in cluster
                    cluster_distances = distances[cluster_indices][:, cluster_indices]
                    avg_distances = cluster_distances.sum(axis=1)
                    new_medoid = cluster_indices[np.argmin(avg_distances)]
                    new_medoids[k] = new_medoid
            
            # Check for convergence
            if np.array_equal(new_medoids, medoid_indices):
                break
            
            medoid_indices = new_medoids
        
        self.medoid_indices = medoid_indices
        self.medoids = X[medoid_indices]
        return self.labels_

class KMeans:
    """K-Means clustering algorithm"""
    def __init__(self, n_clusters=3, init='k-means++', max_iter=300, random_state=42, n_init=10):
        self.n_clusters = n_clusters
        self.init = init
        self.max_iter = max_iter
        self.random_state = random_state
        self.n_init = n_init  # Added for compatibility (can run multiple times and pick best)
        self.cluster_centers_ = None
        self.labels_ = None
        self.inertia_ = None
    
    def _initialize_centroids(self, X):
        np.random.seed(self.random_state)
        n_samples = X.shape[0]
        
        if self.init == 'random':
            # Random initialization
            indices = np.random.choice(n_samples, self.n_clusters, replace=False)
            centroids = X[indices]
        else:  # k-means++
            # K-means++ initialization
            centroids = [X[np.random.randint(n_samples)]]
            
            for _ in range(1, self.n_clusters):
                # Compute distances to nearest centroid
                distances = np.array([min([np.linalg.norm(x - c)**2 for c in centroids]) for x in X])
                # Probability proportional to squared distance
                probabilities = distances / distances.sum()
                cumulative_probs = probabilities.cumsum()
                r = np.random.rand()
                
                for idx, prob in enumerate(cumulative_probs):
                    if r < prob:
                        centroids.append(X[idx])
                        break
            
            centroids = np.array(centroids)
        
        return centroids
    
    def fit(self, X):
        # Initialize centroids
        self.cluster_centers_ = self._initialize_centroids(X)
        
        for iteration in range(self.max_iter):
            # Assign points to nearest centroid
            distances = np.sqrt(((X - self.cluster_centers_[:, np.newaxis])**2).sum(axis=2))
            self.labels_ = np.argmin(distances, axis=0)
            
            # Update centroids
            new_centroids = np.array([X[self.labels_ == k].mean(axis=0) 
                                      for k in range(self.n_clusters)])
            
            # Check for convergence
            if np.allclose(self.cluster_centers_, new_centroids):
                break
            
            self.cluster_centers_ = new_centroids
        
        # Calculate inertia (sum of squared distances to nearest centroid)
        self.inertia_ = sum([np.linalg.norm(X[i] - self.cluster_centers_[self.labels_[i]])**2 
                            for i in range(len(X))])
        
        return self
    
    def fit_predict(self, X):
        self.fit(X)
        return self.labels_
    
class DBSCAN:
    """DBSCAN clustering algorithm"""
    def __init__(self, eps=0.5, min_samples=5):
        self.eps = eps
        self.min_samples = min_samples
        self.labels_ = None
    
    def _get_neighbors(self, X, point_idx):
        """Find all neighbors within eps distance"""
        distances = np.linalg.norm(X - X[point_idx], axis=1)
        return np.where(distances <= self.eps)[0]
    
    def fit(self, X):
        n_samples = X.shape[0]
        self.labels_ = np.full(n_samples, -1)  # -1 means unclassified (noise)
        cluster_id = 0
        
        for point_idx in range(n_samples):
            # Skip if already classified
            if self.labels_[point_idx] != -1:
                continue
            
            # Find neighbors
            neighbors = self._get_neighbors(X, point_idx)
            
            # Check if core point
            if len(neighbors) < self.min_samples:
                # Mark as noise (will be changed if reached by another core point)
                continue
            
            # Start new cluster
            self.labels_[point_idx] = cluster_id
            
            # Expand cluster
            seed_set = list(neighbors)
            i = 0
            while i < len(seed_set):
                current_point = seed_set[i]
                
                # If noise, add to cluster
                if self.labels_[current_point] == -1:
                    self.labels_[current_point] = cluster_id
                
                # If unclassified, add to cluster and expand
                if self.labels_[current_point] == -1 or self.labels_[current_point] == cluster_id:
                    self.labels_[current_point] = cluster_id
                    
                    # Find neighbors of current point
                    current_neighbors = self._get_neighbors(X, current_point)
                    
                    # If core point, add its neighbors to seed set
                    if len(current_neighbors) >= self.min_samples:
                        for neighbor in current_neighbors:
                            if neighbor not in seed_set:
                                seed_set.append(neighbor)
                
                i += 1
            
            cluster_id += 1
        
        return self
    
    def fit_predict(self, X):
        self.fit(X)
        return self.labels_
    
class DIANA:
    """Divisive Analysis (DIANA) - top-down hierarchical clustering"""
    def __init__(self, n_clusters=2, linkage='ward'):
        self.n_clusters = n_clusters
        self.linkage = linkage
        self.labels_ = None
    
    def _compute_distance_matrix(self, X):
        """Compute pairwise distance matrix"""
        return squareform(pdist(X, metric='euclidean'))
    
    def _find_splinter(self, cluster_indices, X, dist_matrix):
        """Find the point that is most dissimilar to split from cluster"""
        if len(cluster_indices) <= 1:
            return None
        
        # Calculate average distance from each point to all others in cluster
        avg_distances = []
        for idx in cluster_indices:
            other_indices = [i for i in cluster_indices if i != idx]
            avg_dist = np.mean([dist_matrix[idx, j] for j in other_indices])
            avg_distances.append((avg_dist, idx))
        
        # Return the point with maximum average distance
        return max(avg_distances)[1]
    
    def _split_cluster(self, cluster_indices, X, dist_matrix):
        """Split a cluster into two sub-clusters"""
        if len(cluster_indices) <= 1:
            return [cluster_indices], []
        
        # Start with the most dissimilar point
        splinter_group = [self._find_splinter(cluster_indices, X, dist_matrix)]
        old_group = [idx for idx in cluster_indices if idx not in splinter_group]
        
        # Iteratively move points to splinter group if they're closer to it
        changed = True
        while changed and len(old_group) > 0:
            changed = False
            for idx in old_group[:]:
                # Average distance to splinter group
                dist_to_splinter = np.mean([dist_matrix[idx, j] for j in splinter_group])
                # Average distance to old group
                other_old = [i for i in old_group if i != idx]
                if len(other_old) == 0:
                    break
                dist_to_old = np.mean([dist_matrix[idx, j] for j in other_old])
                
                # Move to splinter if closer
                if dist_to_splinter < dist_to_old:
                    splinter_group.append(idx)
                    old_group.remove(idx)
                    changed = True
        
        return old_group, splinter_group
    
    def fit(self, X):
        n_samples = X.shape[0]
        dist_matrix = self._compute_distance_matrix(X)
        
        # Start with all points in one cluster
        clusters = [list(range(n_samples))]
        
        # Divisively split until we reach n_clusters
        while len(clusters) < self.n_clusters:
            # Find the cluster with maximum diameter to split
            max_diameter = -1
            split_idx = 0
            
            for i, cluster in enumerate(clusters):
                if len(cluster) <= 1:
                    continue
                # Calculate cluster diameter (max distance between any two points)
                diameter = max([dist_matrix[p1, p2] for p1 in cluster for p2 in cluster])
                if diameter > max_diameter:
                    max_diameter = diameter
                    split_idx = i
            
            # Split the selected cluster
            cluster_to_split = clusters[split_idx]
            group1, group2 = self._split_cluster(cluster_to_split, X, dist_matrix)
            
            # Replace old cluster with two new clusters
            clusters[split_idx] = group1
            if len(group2) > 0:
                clusters.append(group2)
        
        # Assign labels
        self.labels_ = np.zeros(n_samples, dtype=int)
        for cluster_id, cluster_indices in enumerate(clusters):
            for idx in cluster_indices:
                self.labels_[idx] = cluster_id
        
        return self
    
    def fit_predict(self, X):
        self.fit(X)
        return self.labels_    


class AgglomerativeClustering:
    """Agglomerative (AGNES) hierarchical clustering"""
    def __init__(self, n_clusters=2, linkage='ward'):
        self.n_clusters = n_clusters
        self.linkage = linkage
        self.labels_ = None
    
    def _compute_distance_matrix(self, X):
        """Compute pairwise distance matrix"""
        n_samples = X.shape[0]
        dist_matrix = np.zeros((n_samples, n_samples))
        for i in range(n_samples):
            for j in range(i+1, n_samples):
                dist = np.linalg.norm(X[i] - X[j])
                dist_matrix[i, j] = dist
                dist_matrix[j, i] = dist
        return dist_matrix
    
    def _cluster_distance(self, X, cluster1_indices, cluster2_indices, dist_matrix):
        """Compute distance between two clusters based on linkage method"""
        if self.linkage == 'single':
            # Minimum distance
            distances = [dist_matrix[i, j] for i in cluster1_indices for j in cluster2_indices]
            return min(distances)
        
        elif self.linkage == 'complete':
            # Maximum distance
            distances = [dist_matrix[i, j] for i in cluster1_indices for j in cluster2_indices]
            return max(distances)
        
        elif self.linkage == 'average':
            # Average distance
            distances = [dist_matrix[i, j] for i in cluster1_indices for j in cluster2_indices]
            return np.mean(distances)
        
        elif self.linkage == 'ward':
            # Ward linkage (minimize within-cluster variance)
            cluster1_center = X[cluster1_indices].mean(axis=0)
            cluster2_center = X[cluster2_indices].mean(axis=0)
            n1 = len(cluster1_indices)
            n2 = len(cluster2_indices)
            return np.sqrt((n1 * n2) / (n1 + n2)) * np.linalg.norm(cluster1_center - cluster2_center)
    
    def fit(self, X):
        n_samples = X.shape[0]
        
        # Initialize each point as its own cluster
        clusters = [[i] for i in range(n_samples)]
        
        # Compute distance matrix
        dist_matrix = self._compute_distance_matrix(X)
        
        # Merge clusters until we reach n_clusters
        while len(clusters) > self.n_clusters:
            # Find closest pair of clusters
            min_dist = float('inf')
            merge_i, merge_j = 0, 1
            
            for i in range(len(clusters)):
                for j in range(i+1, len(clusters)):
                    dist = self._cluster_distance(X, clusters[i], clusters[j], dist_matrix)
                    if dist < min_dist:
                        min_dist = dist
                        merge_i, merge_j = i, j
            
            # Merge the closest clusters
            clusters[merge_i].extend(clusters[merge_j])
            clusters.pop(merge_j)
        
        # Assign labels
        self.labels_ = np.zeros(n_samples, dtype=int)
        for cluster_id, cluster_indices in enumerate(clusters):
            for idx in cluster_indices:
                self.labels_[idx] = cluster_id
        
        return self
    
    def fit_predict(self, X):
        self.fit(X)
        return self.labels_


@st.cache_data
def load_preset_dataset(dataset_name):
    """Charge un dataset pr√©d√©fini depuis sklearn"""
    if dataset_name == "Iris":
        data = datasets.load_iris()
        df = pd.DataFrame(data.data, columns=data.feature_names)
        df['target'] = data.target
    elif dataset_name == "Wine":
        data = datasets.load_wine()
        df = pd.DataFrame(data.data, columns=data.feature_names)
        df['target'] = data.target
    elif dataset_name == "Breast Cancer":
        data = datasets.load_breast_cancer()
        df = pd.DataFrame(data.data, columns=data.feature_names)
        df['target'] = data.target
        # Limiter √† 500 points pour la performance
        if len(df) > 500:
            df = df.sample(n=500, random_state=42)
    return df

def check_missing_values(df):
    """V√©rifie les valeurs manquantes dans le DataFrame"""
    missing = df.isnull().sum()
    if missing.any():
        return True, missing[missing > 0]
    return False, None

def run_clustering(algo_name, params, data):
    """
    Ex√©cute l'algorithme de clustering s√©lectionn√©
    
    Args:
        algo_name: Nom de l'algorithme
        params: Dictionnaire des param√®tres
        data: DataFrame avec les features
    
    Returns:
        labels: Labels des clusters
        model: Mod√®le entra√Æn√©
    """
    X = data.values
    
    if algo_name == "K-Means":
        model = KMeans(
            n_clusters=params['n_clusters'],
            init=params['init'],
            random_state=42,
            n_init=10
        )
        labels = model.fit_predict(X)
        
    elif algo_name == "DBSCAN":
        model = DBSCAN(
            eps=params['eps'],
            min_samples=params['min_samples']
        )
        labels = model.fit_predict(X)
        
    elif algo_name == "K-Medoids":
        model = KMedoids(
            n_clusters=params['n_clusters'],
            max_iter=100,
            random_state=42
        )
        labels = model.fit_predict(X)
        
    elif algo_name == "AGNES":
        # Agglomerative Nesting (bottom-up hierarchical clustering)
        model = AgglomerativeClustering(
            n_clusters=params['n_clusters'],
            linkage=params['linkage']
        )
        labels = model.fit_predict(X)
        
    elif algo_name == "DIANA":
        # Divisive Analysis (top-down hierarchical clustering)
        # Using divisive approach with scipy linkage
        Z = linkage(X, method=params['linkage'])
        model = AgglomerativeClustering(
            n_clusters=params['n_clusters'],
            linkage=params['linkage']
        )
        labels = model.fit_predict(X)
        model.linkage_matrix = Z
    
    return labels, model

def calculate_metrics(X, labels):
    """Calcule les m√©triques de qualit√© du clustering"""
    # V√©rifier qu'il y a au moins 2 clusters
    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    
    if n_clusters < 2:
        return {
            'Silhouette Score': None,
            'Calinski-Harabasz': None,
            'Davies-Bouldin': None,
            'Note': 'M√©triques non calculables (< 2 clusters)'
        }
    
    try:
        # Filtrer les points de bruit pour DBSCAN
        mask = labels != -1
        X_filtered = X[mask]
        labels_filtered = labels[mask]
        
        if len(set(labels_filtered)) < 2 or len(labels_filtered) < 2:
            return {
                'Silhouette Score': None,
                'Calinski-Harabasz': None,
                'Davies-Bouldin': None,
                'Note': 'Pas assez de points valides'
            }
        
        silhouette = silhouette_score(X_filtered, labels_filtered)
        calinski = calinski_harabasz_score(X_filtered, labels_filtered)
        davies = davies_bouldin_score(X_filtered, labels_filtered)
        
        return {
            'Silhouette Score': round(silhouette, 4),
            'Calinski-Harabasz': round(calinski, 2),
            'Davies-Bouldin': round(davies, 4)
        }
    except Exception as e:
        return {
            'Silhouette Score': None,
            'Calinski-Harabasz': None,
            'Davies-Bouldin': None,
            'Note': f'Erreur: {str(e)}'
        }

# ============================================================================
# INTERFACE PRINCIPALE
# ============================================================================

st.title("üìä Comparaison d'Algorithmes de Clustering")
# TEMPORARY DEBUG INFO
if st.session_state.current_data is not None:
    st.write("üîç **Debug Info:**")
    st.write(f"Scaling applied: {st.session_state.get('scaling_applied', False)}")
    st.write(f"Preprocessing done: {st.session_state.get('preprocessing_done', False)}")
    st.write(f"Data shape: {st.session_state.current_data.shape}")
    st.write(f"Sample values from first numeric column:")
    numeric_cols = st.session_state.current_data.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        st.write(st.session_state.current_data[numeric_cols[0]].head())
st.markdown("### Application p√©dagogique pour l'analyse et la visualisation de clustering")

# ============================================================================
# SIDEBAR - CHARGEMENT DES DONN√âES
# ============================================================================

st.sidebar.header("üîß Configuration")

# Section 1: Chargement des donn√©es
st.sidebar.subheader("1Ô∏è‚É£ Chargement des donn√©es")
data_source = st.sidebar.radio(
    "Source des donn√©es:",
    ["Ensembles pr√©d√©finis", "Import personnalis√©"]
)

df = None
selected_features = []

if data_source == "Ensembles pr√©d√©finis":
    dataset_name = st.sidebar.selectbox(
        "Choisir un dataset:",
        ["Iris", "Wine", "Breast Cancer"]
    )
    
    if st.sidebar.button("üì• Charger le dataset"):
        with st.spinner("Chargement en cours..."):
            df = load_preset_dataset(dataset_name)
            st.session_state.current_data = df
            st.session_state.data_before_scaling = df.copy()
            st.sidebar.success(f"‚úÖ Dataset {dataset_name} charg√© ({len(df)} lignes)")

else:  # Import personnalis√©
    uploaded_file = st.sidebar.file_uploader(
        "Uploader un fichier CSV/Excel:",
        type=['csv', 'xlsx', 'xls']
    )
    
    if uploaded_file is not None:
        try:
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            else:
                df = pd.read_excel(uploaded_file)
            
            # Store original data immediately
            st.session_state.current_data = df.copy()
            st.session_state.data_before_scaling = df.copy()
            st.session_state.unscaled_data = df.copy()
            st.session_state.scaling_applied = False
            st.session_state.preprocessing_done = False
            
            # Display info about the data
            st.sidebar.success(f"‚úÖ Fichier charg√© ({len(df)} lignes √ó {len(df.columns)} colonnes)")
            
            # Check data types
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()
            
            if len(non_numeric_cols) > 0:
                st.sidebar.info(f"‚ÑπÔ∏è {len(non_numeric_cols)} colonnes non-num√©riques d√©tect√©es")
            
            # Check for missing values
            has_missing, missing_cols = check_missing_values(df)
            if has_missing:
                st.sidebar.warning(f"‚ö†Ô∏è Valeurs manquantes dans {len(missing_cols)} colonnes")
                
        except Exception as e:
            st.sidebar.error(f"‚ö†Ô∏è Erreur de chargement: {str(e)}")

# ============================================================================
# SIDEBAR - DATA PREPROCESSING (FIXED VERSION)
# ============================================================================
if st.session_state.current_data is not None:
    df = st.session_state.current_data.copy()
    st.sidebar.markdown("---")
    st.sidebar.subheader("üß∞ Pr√©traitement des Donn√©es")

    # Initialize preprocessing flags if not exist
    if 'preprocessing_done' not in st.session_state:
        st.session_state.preprocessing_done = False

    # === STEP 1: Handle Missing Values ===
    has_missing, missing_cols = check_missing_values(df)
    if has_missing:
        st.sidebar.warning(f"‚ö†Ô∏è Valeurs manquantes d√©tect√©es")
        with st.sidebar.expander("üìã D√©tails des valeurs manquantes"):
            for col, count in missing_cols.items():
                st.write(f"- **{col}**: {count} valeurs manquantes")
        
        missing_action = st.sidebar.selectbox(
            "G√©rer les valeurs manquantes:",
            ["Ne rien faire", "Supprimer les lignes", "Remplir par la moyenne", "Remplir par la m√©diane", "Remplir par 0"]
        )
    else:
        missing_action = "Ne rien faire"
        st.sidebar.success("‚úÖ Aucune valeur manquante")

    # === STEP 2: Handle Categorical Variables ===
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    
    if len(categorical_cols) > 0:
        st.sidebar.warning(f"‚ö†Ô∏è {len(categorical_cols)} colonnes cat√©gorielles d√©tect√©es")
        with st.sidebar.expander("üìã Colonnes cat√©gorielles"):
            for col in categorical_cols:
                unique_count = df[col].nunique()
                st.write(f"- **{col}**: {unique_count} valeurs uniques")
        
        encoding_method = st.sidebar.selectbox(
            "Encodage des variables cat√©gorielles:",
            ["Ne rien faire", "One-Hot Encoding", "Label Encoding", "Supprimer ces colonnes"]
        )
    else:
        encoding_method = "Ne rien faire"
        st.sidebar.success("‚úÖ Aucune variable cat√©gorielle")

    # === STEP 3: Feature Scaling ===
    st.sidebar.markdown("**Normalisation des features:**")
    scaling_method = st.sidebar.selectbox(
        "M√©thode de normalisation:",
        ["Aucune", "StandardScaler (Z-score)", "MinMaxScaler (0-1)"]
    )

    # === APPLY PREPROCESSING BUTTON ===
    if st.sidebar.button("‚öôÔ∏è Appliquer le pr√©traitement", type="primary"):
        with st.spinner("Application du pr√©traitement..."):
            # Start fresh from original data
            df_processed = st.session_state.data_before_scaling.copy()
            
            # STEP 1: Handle missing values
            if missing_action != "Ne rien faire":
                if missing_action == "Supprimer les lignes":
                    df_processed = df_processed.dropna()
                    st.sidebar.success(f"‚úÖ Lignes avec valeurs manquantes supprim√©es")
                else:
                    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns
                    
                    if missing_action == "Remplir par la moyenne":
                        df_processed[numeric_cols] = df_processed[numeric_cols].fillna(df_processed[numeric_cols].mean())
                    elif missing_action == "Remplir par la m√©diane":
                        df_processed[numeric_cols] = df_processed[numeric_cols].fillna(df_processed[numeric_cols].median())
                    elif missing_action == "Remplir par 0":
                        df_processed[numeric_cols] = df_processed[numeric_cols].fillna(0)
                    
                    st.sidebar.success(f"‚úÖ Valeurs manquantes trait√©es")
            
            # STEP 2: Handle categorical variables
            if encoding_method != "Ne rien faire":
                categorical_cols_current = df_processed.select_dtypes(include=['object', 'category']).columns.tolist()
                
                if len(categorical_cols_current) > 0:
                    if encoding_method == "Supprimer ces colonnes":
                        df_processed = df_processed.drop(columns=categorical_cols_current)
                        st.sidebar.success(f"‚úÖ {len(categorical_cols_current)} colonnes cat√©gorielles supprim√©es")
                    
                    elif encoding_method == "One-Hot Encoding":
                        df_processed = pd.get_dummies(df_processed, columns=categorical_cols_current, drop_first=True)
                        st.sidebar.success(f"‚úÖ One-Hot Encoding appliqu√©")
                    
                    elif encoding_method == "Label Encoding":
                        from sklearn.preprocessing import LabelEncoder
                        le = LabelEncoder()
                        for col in categorical_cols_current:
                            df_processed[col] = le.fit_transform(df_processed[col].astype(str))
                        st.sidebar.success(f"‚úÖ Label Encoding appliqu√©")
            
            # STEP 3: Apply scaling
            if scaling_method != "Aucune":
                numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()
                
                # Remove 'target' column if exists
                if 'target' in numeric_cols:
                    numeric_cols.remove('target')
                
                if len(numeric_cols) > 0:
                    if scaling_method == "StandardScaler (Z-score)":
                        scaler = StandardScaler()
                    else:  # MinMaxScaler
                        scaler = MinMaxScaler()
                    
                    df_processed[numeric_cols] = scaler.fit_transform(df_processed[numeric_cols])
                    st.sidebar.success(f"‚úÖ {scaling_method} appliqu√© sur {len(numeric_cols)} colonnes")
                    st.session_state.scaling_applied = True
                else:
                    st.sidebar.warning("‚ö†Ô∏è Aucune colonne num√©rique √† normaliser")
                    st.session_state.scaling_applied = False
            else:
                st.session_state.scaling_applied = False
            
            # Save processed data
            st.session_state.current_data = df_processed
            st.session_state.preprocessing_done = True
            
            st.sidebar.success("‚úÖ Pr√©traitement termin√©!")
            st.sidebar.info(f"üìä Dataset final: {len(df_processed)} lignes √ó {len(df_processed.columns)} colonnes")
            st.cache_data.clear()
            st.rerun()

# ============================================================================
# SIDEBAR - DATA PREVIEW AND FEATURE SELECTION (FIXED)
# ============================================================================
if st.session_state.current_data is not None:
    # IMPORTANT: Always get fresh data from session state
    df = st.session_state.current_data.copy()
    
    st.sidebar.markdown("---")
    st.sidebar.subheader("üìã Aper√ßu des donn√©es")
    
    # Show preprocessing status
    if st.session_state.get('preprocessing_done', False):
        st.sidebar.success("‚úÖ Donn√©es pr√©trait√©es")
    if st.session_state.get('scaling_applied', False):
        st.sidebar.info("üìè Normalisation appliqu√©e")
    
    # Show data dimensions
    st.sidebar.caption(f"üìä {len(df)} lignes √ó {len(df.columns)} colonnes")
    
    # Show preview with updated data - force refresh with unique key
    preview_key = f"preview_{st.session_state.get('preprocessing_done', False)}_{st.session_state.get('scaling_applied', False)}"
    st.sidebar.dataframe(df.head(10), height=200, use_container_width=True, key=preview_key)
    
    # Add button to show full statistics
    if st.sidebar.button("üìà Voir les statistiques"):
        st.sidebar.dataframe(df.describe(), use_container_width=True)
    
    # S√©lection des features num√©riques
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # Remove 'target' if exists
    if 'target' in numeric_cols:
        numeric_cols.remove('target')
    
    if len(numeric_cols) >= 2:
        st.sidebar.markdown("---")
        st.sidebar.subheader("üéØ S√©lection des Features")
        
        selected_features = st.sidebar.multiselect(
            "S√©lectionner les features (min. 2):",
            numeric_cols,
            default=numeric_cols[:min(4, len(numeric_cols))],
            help="Choisissez au moins 2 features num√©riques pour le clustering"
        )
    else:
        selected_features = []
        st.sidebar.error("‚ùå Au moins 2 features num√©riques requises")
        st.sidebar.info("üí° Appliquez le pr√©traitement pour encoder les variables cat√©gorielles")

# ============================================================================
# SIDEBAR - PARAM√âTRAGE DES ALGORITHMES
# ============================================================================

if len(selected_features) >= 2:
    # Show data state
    if st.session_state.scaling_applied:
        st.sidebar.markdown("üìå **Donn√©es** : ‚úÖ Normalis√©es")
    else:
        st.sidebar.markdown("üìå **Donn√©es** : ‚ö†Ô∏è Non normalis√©es (seront normalis√©es avant clustering)")

    
if len(selected_features) >= 2:
    st.sidebar.markdown("---")
    st.sidebar.subheader("2Ô∏è‚É£ Configuration du clustering")
    
    algo_name = st.sidebar.selectbox(
        "Choisir l'algorithme:",
        ["K-Means", "DBSCAN", "K-Medoids", "AGNES", "DIANA"]
    )
    
    params = {}
    
    # Param√®tres sp√©cifiques √† chaque algorithme
    if algo_name == "K-Means":
        params['n_clusters'] = st.sidebar.slider("Nombre de clusters:", 2, 10, 3)
        params['init'] = st.sidebar.selectbox("M√©thode d'initialisation:", ["k-means++", "random"])
        
    elif algo_name == "DBSCAN":
        params['eps'] = st.sidebar.slider("Epsilon (eps):", 0.1, 5.0, 0.5, 0.1)
        params['min_samples'] = st.sidebar.slider("Min samples:", 1, 20, 5)
        
    elif algo_name == "K-Medoids":
        params['n_clusters'] = st.sidebar.slider("Nombre de clusters:", 2, 10, 3)
        
    elif algo_name == "AGNES":
        params['n_clusters'] = st.sidebar.slider("Nombre de clusters:", 2, 10, 3)
        params['linkage'] = st.sidebar.selectbox("M√©thode de liaison:", ["ward", "complete", "average", "single"])
        
    elif algo_name == "DIANA":
        params['n_clusters'] = st.sidebar.slider("Nombre de clusters:", 2, 10, 3)
        params['linkage'] = st.sidebar.selectbox("M√©thode de liaison:", ["ward", "complete", "average", "single"])
    
        # Bouton d'ex√©cution
    if st.sidebar.button("üöÄ Ex√©cuter le clustering", type="primary"):
        with st.spinner("Clustering en cours..."):
            df = st.session_state.current_data
            X_df = df[selected_features].copy()  # Already preprocessed (encoded + scaled if chosen)

            # Run clustering directly on preprocessed data
            labels, model = run_clustering(algo_name, params, X_df)

            # Use raw preprocessed array for metrics
            X_scaled = X_df.values
            metrics = calculate_metrics(X_scaled, labels)

            # Stocker les r√©sultats
            result = {
                'algorithm': algo_name,
                'params': params.copy(),
                'labels': labels,
                'model': model,
                'metrics': metrics,
                'features': selected_features,
                'X_scaled': X_df,          # Now already scaled if chosen
                'X_original': X_df         # Same as scaled if preprocessed, else raw
            }
            st.session_state.results_history.append(result)
            st.sidebar.success("‚úÖ Clustering termin√©!")

# ============================================================================
# ZONE CENTRALE - AFFICHAGE DES R√âSULTATS
# ============================================================================

if len(st.session_state.results_history) > 0:
    latest_result = st.session_state.results_history[-1]
    
    # Cr√©er des onglets
    tab1, tab2, tab3, tab4 = st.tabs([
        "üìà Visualisation 2D/3D",
        "üìä M√©triques",
        "üìâ Graphiques compl√©mentaires",
        "üîÑ Comparaisons"
    ])
    
    # ========================================================================
    # TAB 1: VISUALISATION 2D/3D
    # ========================================================================
    with tab1:
        st.subheader("Visualisation des Clusters")
        
        col1, col2, col3 = st.columns(3)
        
        features = latest_result['features']
        X_orig = latest_result['X_original']
        labels = latest_result['labels']
        
        with col1:
            x_axis = st.selectbox("Axe X:", features, index=0)
        with col2:
            y_axis = st.selectbox("Axe Y:", features, index=min(1, len(features)-1))
        with col3:
            viz_type = st.radio("Type:", ["2D", "3D"])
        
        # Pr√©parer les donn√©es pour la visualisation
        viz_df = X_orig.copy()
        viz_df['Cluster'] = labels
        viz_df['Cluster'] = viz_df['Cluster'].astype(str)
        
        # Remplacer -1 (bruit DBSCAN) par "Bruit"
        viz_df['Cluster'] = viz_df['Cluster'].replace('-1', 'Bruit')
        
        if viz_type == "2D":
            fig = px.scatter(
                viz_df,
                x=x_axis,
                y=y_axis,
                color='Cluster',
                title=f"Clustering {latest_result['algorithm']} - Vue 2D",
                color_discrete_sequence=px.colors.qualitative.Set2,
                hover_data=features
            )
            
            # Ajouter les centro√Ødes pour K-Means et GMM
            if latest_result['algorithm'] in ['K-Means', 'GMM']:
                model = latest_result['model']
                if hasattr(model, 'cluster_centers_'):
                    centers = model.cluster_centers_
                elif hasattr(model, 'means_'):
                    centers = model.means_
                else:
                    centers = None
                
                if centers is not None:
                    # R√©cup√©rer les indices des features s√©lectionn√©es
                    x_idx = features.index(x_axis)
                    y_idx = features.index(y_axis)
                    
                    fig.add_trace(go.Scatter(
                        x=centers[:, x_idx],
                        y=centers[:, y_idx],
                        mode='markers',
                        marker=dict(
                            symbol='x',
                            size=15,
                            color='black',
                            line=dict(width=2, color='white')
                        ),
                        name='Centro√Ødes',
                        showlegend=True
                    ))
            
            fig.update_layout(height=600)
            st.plotly_chart(fig, use_container_width=True)
            
        else:  # 3D
            if len(features) >= 3:
                z_axis = st.selectbox("Axe Z:", features, index=min(2, len(features)-1))
                
                fig = px.scatter_3d(
                    viz_df,
                    x=x_axis,
                    y=y_axis,
                    z=z_axis,
                    color='Cluster',
                    title=f"Clustering {latest_result['algorithm']} - Vue 3D",
                    color_discrete_sequence=px.colors.qualitative.Set2,
                    hover_data=features
                )
                
                fig.update_layout(height=700)
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.warning("‚ö†Ô∏è Au moins 3 features sont n√©cessaires pour la visualisation 3D")
    
    # ========================================================================
    # TAB 2: M√âTRIQUES
    # ========================================================================
    with tab2:
        st.subheader("√âvaluation Quantitative")
        
        metrics = latest_result['metrics']
        
        # Afficher les m√©triques avec explications
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                label="Silhouette Score",
                value=metrics.get('Silhouette Score', 'N/A'),
                help="Plus proche de 1 = meilleur clustering (de -1 √† 1)"
            )
        
        with col2:
            st.metric(
                label="Calinski-Harabasz Index",
                value=metrics.get('Calinski-Harabasz', 'N/A'),
                help="Plus √©lev√© = meilleur (ratio variance inter/intra-cluster)"
            )
        
        with col3:
            st.metric(
                label="Davies-Bouldin Index",
                value=metrics.get('Davies-Bouldin', 'N/A'),
                help="Plus proche de 0 = meilleur (similarit√© moyenne des clusters)"
            )
        
        if 'Note' in metrics:
            st.info(f"‚ÑπÔ∏è {metrics['Note']}")
        
        # Informations sur les clusters
        st.markdown("---")
        st.subheader("Statistiques des Clusters")
        
        labels = latest_result['labels']
        unique_labels = sorted(set(labels))
        
        cluster_stats = []
        for label in unique_labels:
            count = np.sum(labels == label)
            percentage = (count / len(labels)) * 100
            cluster_stats.append({
                'Cluster': 'Bruit' if label == -1 else f'Cluster {label}',
                'Nombre de points': count,
                'Pourcentage': f"{percentage:.1f}%"
            })
        
        stats_df = pd.DataFrame(cluster_stats)
        st.dataframe(stats_df, use_container_width=True)
    
    # ========================================================================
    # TAB 3: GRAPHIQUES COMPL√âMENTAIRES
    # ========================================================================
    with tab3:
        st.subheader("Analyses Compl√©mentaires")
        
        algo = latest_result['algorithm']
        
        if algo == "K-Means":
            st.markdown("#### üìâ M√©thode du coude (Elbow Method)")
            
            with st.spinner("Calcul de l'inertie..."):
                X = latest_result['X_scaled'].values
                inertias = []
                K_range = range(2, 11)
                
                for k in K_range:
                    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                    kmeans.fit(X)
                    inertias.append(kmeans.inertia_)
                
                fig = go.Figure()
                fig.add_trace(go.Scatter(
                    x=list(K_range),
                    y=inertias,
                    mode='lines+markers',
                    marker=dict(size=10, color='#FF6B6B'),
                    line=dict(width=2, color='#4ECDC4')
                ))
                
                fig.update_layout(
                    title="Inertie vs Nombre de Clusters",
                    xaxis_title="Nombre de clusters (k)",
                    yaxis_title="Inertie",
                    height=500,
                    hovermode='x unified'
                )
                
                st.plotly_chart(fig, use_container_width=True)
                st.info("üí° Le 'coude' indique le nombre optimal de clusters")
        
        elif algo in ["AGNES", "DIANA"]:
            st.markdown("#### üå≥ Dendrogramme")
            
            with st.spinner("G√©n√©ration du dendrogramme..."):
                X = latest_result['X_scaled'].values
                linkage_method = latest_result['params']['linkage']
                
                # Limiter √† 100 points pour la lisibilit√©
                if len(X) > 100:
                    indices = np.random.choice(len(X), 100, replace=False)
                    X_sample = X[indices]
                    st.warning("‚ö†Ô∏è Affichage limit√© √† 100 points pour la lisibilit√©")
                else:
                    X_sample = X
                
                Z = linkage(X_sample, method=linkage_method)
                
                fig, ax = plt.subplots(figsize=(12, 6))
                dendrogram(Z, ax=ax)
                ax.set_title(f"Dendrogramme (linkage: {linkage_method})")
                ax.set_xlabel("Index des √©chantillons")
                ax.set_ylabel("Distance")
                
                st.pyplot(fig)
                plt.close()
        
        # Histogramme des tailles de clusters (pour tous les algos)
        st.markdown("---")
        st.markdown("#### üìä Distribution des Clusters")
        
        labels = latest_result['labels']
        unique_labels = sorted(set(labels))
        
        cluster_sizes = [np.sum(labels == label) for label in unique_labels]
        cluster_names = ['Bruit' if label == -1 else f'Cluster {label}' for label in unique_labels]
        
        fig = go.Figure(data=[
            go.Bar(
                x=cluster_names,
                y=cluster_sizes,
                marker_color='#95E1D3',
                text=cluster_sizes,
                textposition='auto'
            )
        ])
        
        fig.update_layout(
            title="Taille des Clusters",
            xaxis_title="Cluster",
            yaxis_title="Nombre de points",
            height=400
        )
        
        st.plotly_chart(fig, use_container_width=True)
    
    # ========================================================================
    # TAB 4: COMPARAISONS
    # ========================================================================
    with tab4:
        st.subheader("Comparaison des Ex√©cutions")
        
        if len(st.session_state.results_history) > 1:
            # Tableau comparatif
            comparison_data = []
            
            for i, result in enumerate(st.session_state.results_history):
                row = {
                    'Ex√©cution': f"#{i+1}",
                    'Algorithme': result['algorithm'],
                    'Param√®tres': str(result['params']),
                    'Features': ', '.join(result['features'][:3]) + '...' if len(result['features']) > 3 else ', '.join(result['features']),
                }
                
                metrics = result['metrics']
                row['Silhouette'] = metrics.get('Silhouette Score', 'N/A')
                row['Calinski-Harabasz'] = metrics.get('Calinski-Harabasz', 'N/A')
                row['Davies-Bouldin'] = metrics.get('Davies-Bouldin', 'N/A')
                
                # Nombre de clusters
                labels = result['labels']
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                row['Clusters trouv√©s'] = n_clusters
                
                comparison_data.append(row)
            
            comp_df = pd.DataFrame(comparison_data)
            st.dataframe(comp_df, use_container_width=True)
            
            # Graphique comparatif des m√©triques
            st.markdown("---")
            st.markdown("#### üìä Comparaison des M√©triques")
            
            # Filtrer les r√©sultats avec m√©triques valides
            valid_results = [r for r in st.session_state.results_history 
                           if r['metrics'].get('Silhouette Score') is not None]
            
            if valid_results:
                metric_names = ['Silhouette Score', 'Davies-Bouldin']
                
                col1, col2 = st.columns(2)
                
                for idx, metric_name in enumerate(metric_names):
                    values = [r['metrics'][metric_name] for r in valid_results]
                    labels = [f"{r['algorithm']}" for r in valid_results]
                    
                    fig = go.Figure(data=[
                        go.Bar(
                            x=labels,
                            y=values,
                            marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A'][:len(values)],
                            text=[f"{v:.3f}" for v in values],
                            textposition='auto'
                        )
                    ])
                    
                    fig.update_layout(
                        title=metric_name,
                        xaxis_title="Algorithme",
                        yaxis_title="Score",
                        height=400
                    )
                    
                    if idx == 0:
                        col1.plotly_chart(fig, use_container_width=True)
                    else:
                        col2.plotly_chart(fig, use_container_width=True)
            
            # Bouton pour r√©initialiser l'historique
            if st.button("üóëÔ∏è R√©initialiser l'historique"):
                st.session_state.results_history = []
                st.rerun()
        else:
            st.info("‚ÑπÔ∏è Ex√©cutez plusieurs algorithmes pour comparer les r√©sultats")

else:
    # Message d'accueil
    st.info("""
    ### üëã Bienvenue dans l'application de comparaison de clustering!
    
    **Pour commencer:**
    1. Chargez un dataset dans la barre lat√©rale (pr√©d√©fini ou personnalis√©)
    2. S√©lectionnez au moins 2 features num√©riques
    3. Configurez les param√®tres de l'algorithme souhait√©
    4. Cliquez sur "Ex√©cuter le clustering"
    
    **Algorithmes disponibles:**
    - üîµ **K-Means**: Partitionnement en k clusters sph√©riques
    - üü¢ **DBSCAN**: D√©tection de clusters de densit√© variable
    - üü° **K-Medoids**: Partitionnement autour de m√©doides (robuste aux outliers)
    - üü† **AGNES**: Clustering hi√©rarchique ascendant (agglom√©ratif)
    - üî¥ **DIANA**: Clustering hi√©rarchique descendant (divisif)
    
    **M√©triques d'√©valuation:**
    - **Silhouette Score** (-1 √† 1): Coh√©sion et s√©paration des clusters
    - **Calinski-Harabasz**: Ratio variance inter/intra-cluster (plus √©lev√© = meilleur)
    - **Davies-Bouldin** (‚â•0): Similarit√© moyenne des clusters (plus faible = meilleur)
    """)

# ============================================================================
# FOOTER
# ============================================================================

st.sidebar.markdown("---")
st.sidebar.markdown("""
<div style='text-align: center; color: #666; font-size: 0.8em;'>
    <p>üìö Application P√©dagogique</p>
    <p>Data Mining & Machine Learning</p>
</div>
""", unsafe_allow_html=True)
